# No-Limit-Texas-Hold-em
In this project, we conduct an ablation study comparing two variants of the Proximal Policy Optimization (PPO) algorithm—PPO-Clip and PPO-KL—within the context of No-Limit Texas Hold'em using the RLCard environment. We develop a rich state representation, implement domain-specific reward shaping, and introduce a self-play training framework to expose agents to a curriculum of increasingly sophisticated opponents. Our results show that PPO-KL converges to a stable, exploitative strategy with a high win rate but lower average rewards, while PPO-Clip continues exploring diverse strategies and achieves higher reward peaks at the cost of consistency. This study illustrates how different constraint mechanisms in PPO shape learning dynamics and strategic behavior in complex, imperfect-information games like poker.
